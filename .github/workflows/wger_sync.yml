name: Wger Workout Sync

on:
  schedule:
    - cron: "45 3 * * *" # Runs daily
  workflow_dispatch:

permissions:
  contents: write

jobs:
  sync-wger:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install requests python-dotenv

      - name: Fetch and process wger workout data
        env:
          WGER_API_KEY: ${{ secrets.WGER_API_KEY }}
          WORKOUT_LOG_CSV: knowledge/workout_log.csv
        run: |
          python - <<'PY'
          import os
          import json
          import csv
          from pathlib import Path
          import requests

          API_KEY = os.getenv("WGER_API_KEY")
          CSV_PATH = Path(os.getenv("WORKOUT_LOG_CSV", "knowledge/workout_log.csv"))

          if not API_KEY:
              print("âŒ WGER_API_KEY secret not found.")
              exit(1)

          def fetch_paginated_data(url, headers):
              """Fetches all pages for a given API endpoint."""
              all_results = []
              while url:
                  try:
                      response = requests.get(url, headers=headers)
                      response.raise_for_status()
                      data = response.json()
                      all_results.extend(data.get('results', []))
                      url = data.get('next')
                  except requests.exceptions.RequestException as e:
                      print(f"Error fetching {url}: {e}")
                      return None
              return all_results

          def main():
              print("ðŸš€ Starting wger sync...")
              headers = {"Authorization": f"Token {API_KEY}"}

              # --- 1. Fetch all exercises to create a lookup table ---
              print("Fetching exercise database for enrichment...")
              exercise_info_url = "https://wger.de/api/v2/exerciseinfo/?language=2"
              all_exercises = fetch_paginated_data(exercise_info_url, headers)
              
              if all_exercises is None:
                  print("âŒ Failed to fetch exercise database. Aborting.")
                  exit(1)

              exercise_lookup = {
                  exercise['id']: {
                      'name': exercise['name'],
                      'category': exercise.get('category', {}).get('name', 'N/A')
                  } for exercise in all_exercises
              }
              print(f"Created lookup for {len(exercise_lookup)} exercises.")

              # --- 2. Fetch Executed Workout Logs ---
              print("Fetching executed workout logs...")
              logs_url = "https://wger.de/api/v2/log/"
              all_logs = fetch_paginated_data(logs_url, headers)

              if all_logs is None:
                  print("âŒ Failed to fetch workout logs. Aborting.")
                  exit(1)
              
              if not all_logs:
                  print("No workout logs found on wger.")
                  return

              # --- 3. Process Logs into CSV format ---
              CSV_PATH.parent.mkdir(parents=True, exist_ok=True)
              header = ['date', 'exercise_name', 'category', 'reps', 'weight_kg']
              
              processed_logs = []
              for log in all_logs:
                  exercise_details = exercise_lookup.get(log['exercise'], {})
                  processed_logs.append({
                      'date': log['date'],
                      'exercise_name': exercise_details.get('name', f"Unknown ID: {log['exercise']}"),
                      'category': exercise_details.get('category', 'N/A'),
                      'reps': log['reps'],
                      'weight_kg': log['weight']
                  })

              # --- 4. Write to CSV, avoiding duplicates ---
              existing_logs = set()
              if CSV_PATH.exists():
                  with open(CSV_PATH, 'r', newline='', encoding='utf-8') as f:
                      reader = csv.DictReader(f)
                      for row in reader:
                          existing_logs.add(tuple(row.values()))
              
              new_rows = []
              for log in sorted(processed_logs, key=lambda x: x['date']):
                  log_tuple = tuple(str(v) for v in log.values())
                  if log_tuple not in existing_logs:
                      new_rows.append(log)
                      existing_logs.add(log_tuple)

              if new_rows:
                  print(f"Adding {len(new_rows)} new workout sets to {CSV_PATH}")
                  # Open in append mode, write header only if file is new
                  is_new_file = not CSV_PATH.exists()
                  with open(CSV_PATH, 'a', newline='', encoding='utf-8') as f:
                      writer = csv.DictWriter(f, fieldnames=header)
                      if is_new_file or f.tell() == 0:
                          writer.writeheader()
                      writer.writerows(new_rows)
              else:
                  print("No new workout sets to add.")

              print("âœ… Sync complete.")

          if __name__ == "__main__":
              main()
          PY
      - name: Commit updates
        run: |
          if [[ -n "$(git status --porcelain knowledge/workout_log.csv 2>/dev/null)" ]]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add knowledge/workout_log.csv
            git commit -m "chore(wger): sync workout data"
            git push
          else
            echo "No changes to commit from wger sync."
          fi
